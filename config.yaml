spot_mode: false
persistent_storage_path: "/persistent"

data:
  raw_dir: "data/raw"
  processed_dir: "data/processed"
  checkpoints: "data/checkpoints"
  results: "data/results"
  sample_ratio: 1
  k_hop: 1

  columns:
    nodes:
      node_id: "clId"
      subgraph_id: "ccId"
    edges:
      source: "clId1"
      target: "clId2"
      txId: "txId"
    components:
      subgraph_id: "ccId"
      label: "ccLabel"
    background_nodes:
      node_id: "clId"
    background_edges:
      txId: "txId"
      source: "clId1"
      target: "clId2"

model:
  hidden_dim: 128
  num_layers: 3
  heads: 4
  edge_proj_dim: 32
  dropout: 0.4
  num_classes: 2

training:
  epochs: 300
  batch_size: 256
  learning_rate: 0.001
  warmup_epochs: 5
  weight_decay: 0.0001
  lr_schedule: "cosine"    # "plateau" (ReduceLROnPlateau) or "cosine" (CosineAnnealingLR)
  lr_patience: 15          # only used with lr_schedule: "plateau"
  lr_factor: 0.5           # only used with lr_schedule: "plateau"
  early_stop_patience: 40
  cost_matrix:
    fn_cost: 25.0
    fp_cost: 1.0

  label_smoothing: 0.05    # Smooth targets toward uniform (0.0 = off, 0.05 = recommended)
  calibration: true        # Post-hoc temperature scaling on validation set

  class_weighting: false   # OFF â€” balanced_sampling already handles imbalance; both together causes NaN
  oversampling: false      # ignored when balanced_sampling=true, explicit for clarity
  balanced_sampling: true  # Use BalancedBatchSampler instead of WeightedRandomSampler

  seeds: [42, 123, 456]  # Multi-seed training for statistical significance

  val_ratio: 0.15
  test_ratio: 0.10

  checkpoint_dir: "checkpoints"
  save_every: 1

device: "cuda"
